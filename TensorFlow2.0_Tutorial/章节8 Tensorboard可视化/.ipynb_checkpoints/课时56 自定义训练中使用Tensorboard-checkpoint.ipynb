{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 课时56 自定义训练中使用Tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow Version: 2.0.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sb\n",
    "sb.set_style('darkgrid')\n",
    "# pathlib相比os.path更好用\n",
    "import pathlib\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import glob\n",
    "print('Tensorflow Version:', tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. 自定义训练添加Tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<TensorSliceDataset shapes: ((28, 28, 1), ()), types: (tf.float32, tf.int64)>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 加载MNIST数据集\n",
    "(train_images, train_labels), (test_images, test_labels) = tf.keras.datasets.mnist.load_data()\n",
    "# 以tf.Dataset形式加载数据(对于MNIST数据集，它是没有第三个维度Channel的，这里添加上它的第三个维度)\n",
    "train_images = tf.expand_dims(train_images, -1)\n",
    "# 由于tf.GradientTape()要求的数据类型是float，因此这里需要改变MNIST数据集图片的数据类型\n",
    "# 并在转换数据类型的过程中对图片数据进行归一化\n",
    "train_images = tf.cast(train_images/255, tf.float32)\n",
    "# 对于MNIST数据集的标签，数据类型是uint8(无符号8位)，这里为了方便计算，将其转换为int64类型\n",
    "train_labels = tf.cast(train_labels, tf.int64)\n",
    "dataset = tf.data.Dataset.from_tensor_slices((train_images, train_labels))\n",
    "\n",
    "test_images = tf.expand_dims(test_images, -1)\n",
    "test_images = tf.cast(test_images/255, tf.float32)\n",
    "test_labels = tf.cast(test_labels, tf.int64)\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((test_images, test_labels))\n",
    "test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset shapes: ((None, 28, 28, 1), (None,)), types: (tf.float32, tf.int64)>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 对数据进行打乱以及batch划分\n",
    "dataset = dataset.shuffle(buffer_size=10000).batch(batch_size=32)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset shapes: ((None, 28, 28, 1), (None,)), types: (tf.float32, tf.int64)>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset = test_dataset.batch(batch_size=32)\n",
    "test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 26, 26, 16)        160       \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 24, 24, 32)        4640      \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d (Gl (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 10)                330       \n",
      "=================================================================\n",
      "Total params: 5,130\n",
      "Trainable params: 5,130\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 建立模型\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Conv2D(filters=16, kernel_size=[3, 3], activation='relu', input_shape=(28, 28, 1)),\n",
    "    tf.keras.layers.Conv2D(filters=32, kernel_size=[3, 3], activation='relu'),\n",
    "    tf.keras.layers.GlobalAveragePooling2D(),\n",
    "    tf.keras.layers.Dense(units=10, activation='softmax')])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 进行自定义训练\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "loss_func = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "# 由于dataset是tf.Dataset对象，因此在eager模型下是直接可迭代的(这里设定了batch=32，因此每次迭代都取出32个数据)：\n",
    "features, labels = next(iter(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=157, shape=(32,), dtype=int64, numpy=\n",
       "array([3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "       3, 3, 3, 3, 3, 3, 3, 3, 3, 3], dtype=int64)>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model内置call方法，因此可以直接调用\n",
    "predictions = model(features)\n",
    "# 还没进行训练的时候预测的进度不好\n",
    "tf.argmax(predictions, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义损失函数，每次传入数据之后都会返回相应的损失值\n",
    "def loss(model, x, y):\n",
    "    y_ = model(x)\n",
    "    return loss_func(y, y_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义两个tf.keras.metrics对象用于计算自定义训练中的loss均值和正确率\n",
    "train_loss = tf.keras.metrics.Mean('train_loss')\n",
    "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy('train_accuracy')\n",
    "\n",
    "test_loss = tf.keras.metrics.Mean('test_loss')\n",
    "test_accuracy = tf.keras.metrics.SparseCategoricalAccuracy('test_accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 计算一步对应的损失函数与模型参数的梯度\n",
    "def train_step(model, images, labels):\n",
    "    with tf.GradientTape() as t:\n",
    "        # loss_step = loss(model, images, labels)\n",
    "        # 为了方便tf.keras.metrics模块的演示，将上面这个代码拆成两步，不再调用loss计算函数\n",
    "        pred = model(images)\n",
    "        loss_step = loss_func(labels, pred)\n",
    "    grads = t.gradient(loss_step, model.trainable_variables)\n",
    "    # apply_gradients()代表将优化方法应用到参数的更新中\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "    # 这里是新增的部分，每一步都计算均值和正确率\n",
    "    train_loss(loss_step)\n",
    "    train_accuracy(labels, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 计算一步对应的损失函数与模型参数的梯度\n",
    "def test_step(model, images, labels):\n",
    "    pred = model(images)\n",
    "    loss_step = loss_func(labels, pred)\n",
    "    test_loss(loss_step)\n",
    "    test_accuracy(labels, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建filewriter\n",
    "import datetime\n",
    "import os\n",
    "# current_time = datetime.datetime.now().strftime('%Y%M%D')\n",
    "train_log_dir = os.path.join('logss', 'train')\n",
    "test_log_dir = os.path.join('logss', 'test')\n",
    "\n",
    "train_writer = tf.summary.create_file_writer(logdir=train_log_dir)\n",
    "test_writer = tf.summary.create_file_writer(logdir=test_log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model():\n",
    "    # 训练3个epochs\n",
    "    for epoch in range(3):\n",
    "        for (batch, (images, labels)) in enumerate(dataset):\n",
    "            train_step(model, images, labels)\n",
    "            \n",
    "        with train_writer.as_default():\n",
    "            tf.summary.scalar('train_loss', train_loss.result(), step=epoch)\n",
    "            tf.summary.scalar('train_acc', train_accuracy.result(), step=epoch)\n",
    "        \n",
    "        print('Epoch %i loss is %f, acc is %f'%(epoch, \n",
    "                                                train_loss.result(),\n",
    "                                                train_accuracy.result()))\n",
    "        # 每个batch结束就重置这一轮的train_loss和train_accuracy\n",
    "        train_loss.reset_states()\n",
    "        train_accuracy.reset_states()\n",
    "        \n",
    "        # =====================================================================\n",
    "        for (batch, (images, labels)) in enumerate(test_dataset):\n",
    "            test_step(model, images, labels)\n",
    "        \n",
    "        with train_writer.as_default():\n",
    "            tf.summary.scalar('test_loss', test_loss.result(), step=epoch)\n",
    "            tf.summary.scalar('test_acc', test_accuracy.result(), step=epoch)\n",
    "        \n",
    "        print('Epoch %i test_loss is %f, test_acc is %f'%(epoch, \n",
    "                                                test_loss.result(),\n",
    "                                                test_accuracy.result()))\n",
    "        \n",
    "        # 每个batch结束就重置这一轮的train_loss和train_accuracy\n",
    "        test_loss.reset_states()\n",
    "        test_accuracy.reset_states()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 loss is 2.157457, acc is 0.307167\n",
      "Epoch 0 test_loss is 2.085262, test_acc is 0.376000\n",
      "Epoch 1 loss is 2.044552, acc is 0.430450\n",
      "Epoch 1 test_loss is 2.009225, test_acc is 0.465300\n",
      "Epoch 2 loss is 1.985295, acc is 0.495200\n",
      "Epoch 2 test_loss is 1.954638, test_acc is 0.524500\n"
     ]
    }
   ],
   "source": [
    "train_model()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
